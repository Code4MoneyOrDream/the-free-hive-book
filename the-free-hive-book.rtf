{\rtf1\ansi\ansicpg1252\cocoartf1138\cocoasubrtf510
{\fonttbl\f0\fnil\fcharset0 HelveticaNeue;\f1\fmodern\fcharset0 Courier;}
{\colortbl;\red255\green255\blue255;\red14\green14\blue14;\red22\green88\blue148;\red255\green255\blue255;
\red0\green0\blue0;}
{\*\listtable{\list\listtemplateid1\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{decimal\}.}{\leveltext\leveltemplateid1\'02\'00.;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{circle\}}{\leveltext\leveltemplateid2\'01\uc0\u9702 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listname ;}\listid1}
{\list\listtemplateid2\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid101\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid2}}
{\*\listoverridetable{\listoverride\listid1\listoverridecount0\ls1}{\listoverride\listid2\listoverridecount0\ls2}}
{\info
{\title the-free-hive-book}}\deftab720
\pard\pardeftab720\sl380

\f0\fs26 \cf2 \pard\pardeftab720\sl880\sa380

\b\fs60 \cf0 \
The 
\i Free
\i0  Hive Book\
\pard\pardeftab720\sl380\sa380

\b0\fs30 \cf2 by {\field{\*\fldinst{HYPERLINK "http://www.semantikoz.com/blog/"}}{\fldrslt \cf3 Christian Prokopp}}, {\field{\*\fldinst{HYPERLINK "http://www.twitter.com/prokopp"}}{\fldrslt \cf3 @prokopp}}, {\field{\*\fldinst{HYPERLINK "https://plus.google.com/106245424622248310303?rel=author"}}{\fldrslt \cf3 Google+}}\
\pard\pardeftab720\sl380\sa120

\fs26 \cf2 \
\pard\pardeftab720\sl880\sa380

\b\fs60 \cf0 About This Book\
\pard\pardeftab720\sl660\sa380

\fs44 \cf2 License\
\pard\pardeftab720\sl380\sa380

\b0\fs30 \cf2 The Little Apache Hive Book is licensed under the {\field{\*\fldinst{HYPERLINK "https://creativecommons.org/licenses/by-nc/3.0/"}}{\fldrslt \cf3 Attribution-NonCommercial 3.0 Unported license}}.\
You are free to share (copy, distribute and transmit the book), you can change the book (extend, fix, shorten, translate, \'85). However, you need to attribute the work to the author, {\field{\*\fldinst{HYPERLINK "http://www.semantikoz.com/"}}{\fldrslt \cf3 Christian Prokopp}} {\field{\*\fldinst{HYPERLINK "http://www.twitter.com/prokopp"}}{\fldrslt \cf3 @prokopp}}. Lastly, you can not use the work commercially. You can contact me if you like to do so though.\
Thank you.\
\pard\pardeftab720\sl660\sa380

\b\fs44 \cf2 About The Author\
\pard\pardeftab720\sl380\sa380
{\field{\*\fldinst{HYPERLINK "http://uk.linkedin.com/in/prokopp"}}{\fldrslt 
\b0\fs30 \cf3 Christian Prokopp}}
\b0\fs30  is a Data Scientist at {\field{\*\fldinst{HYPERLINK "http://www.rangespan.com/"}}{\fldrslt \cf3 Rangespan}}, and writes as {\field{\*\fldinst{HYPERLINK "http://www.semantikoz.com/blog"}}{\fldrslt \cf3 Blogger and Data Journalist}} in his spare time. Christian holds a BSc, MCom, PhD and has lived, worked and researched in three continents.\
\pard\pardeftab720\sl660\sa380

\b\fs44 \cf2 Why this book?\
\pard\pardeftab720\sl380\sa380

\b0\fs30 \cf2 This book was sparked by the need to give some tutorial material to business users at Rangespan. Christian has been working with Hive and Hadoop for the last two years. Giving access to Big Data to business stakeholder through a SQL-like interface has proved extremely valuable and popular. Facebook, a company with one of the largest data sets world-wide for example, heavily relies on Hive and Hadoop for data processing, insight and reporting with thousands of users accessing data spread across thousands of computers.\
In that spirit this book provides an example driven introduction to working with Hive. The book helps SQL experienced users to apply their knowledge when working with Hive. At the end of the book are also some more advanced tips for power-users, dev-ops or sysadmins.\
\pard\pardeftab720\sl660\sa380

\b\fs44 \cf2 Original and Latest Version\
\pard\pardeftab720\sl380\sa380

\b0\fs30 \cf2 The original with the latest updates is available as the {\field{\*\fldinst{HYPERLINK "http://www.semantikoz.com/blog/the-free-apache-hive-book/"}}{\fldrslt \cf3 The Free Hive Book}}.\
\
\pard\pardeftab720\sl880\sa380

\b\fs60 \cf0 Table of Contents\
\pard\pardeftab720\sl380\sa380

\b0\fs30 \cf2 The book is work in progress and the TOC as well as the actual chapters will evolve.\
\pard\tx220\tx720\pardeftab720\li720\fi-720\sl360
\ls1\ilvl0
\b\fs28 \cf3 {\listtext	1.	}Introduction\cf2  - What is Hive
\b0 \
\ls1\ilvl0
\b \cf3 {\listtext	2.	}Getting Started\cf2  - Setup Hive
\b0 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\sl380
\ls1\ilvl0
\b \cf3 {\listtext	3.	}Create, Load, Select, Drop\cf2  - The basics
\b0 \
\pard\tx940\tx1440\pardeftab720\li1440\fi-1440\sl380
\ls1\ilvl1
\i \cf2 {\listtext	\uc0\u9702 	}Managed Table
\i0 \
\ls1\ilvl1
\i {\listtext	\uc0\u9702 	}External Table
\i0 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\sl360
\ls1\ilvl0
\b \cf3 {\listtext	4.	}Setting up the example table
\b0 \cf2 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\sl380
\ls1\ilvl0
\b \cf3 {\listtext	5.	}Query
\b0 \cf2 \
\pard\tx940\tx1440\pardeftab720\li1440\fi-1440\sl380
\ls1\ilvl1
\i \cf2 {\listtext	\uc0\u9702 	}SELECT \'85 WHERE \'85
\i0 \
\ls1\ilvl1
\i {\listtext	\uc0\u9702 	}SELECT \'85 ORDER BY \'85
\i0 \
\ls1\ilvl1
\i {\listtext	\uc0\u9702 	}SELECT \'85 SORT BY \'85
\i0 \
\ls1\ilvl1
\i {\listtext	\uc0\u9702 	}SELECT \'85 CLUSTER BY \'85
\i0 \
\ls1\ilvl1
\i {\listtext	\uc0\u9702 	}SELECT \'85 DISTRIBUTE BY \'85
\i0 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\sl360
\ls1\ilvl0
\b \cf2 {\listtext	6.	}Normalize Tables
\b0 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\sl380
\ls1\ilvl0
\b \cf3 {\listtext	7.	}Joining Tables
\b0 \cf2 \
\pard\tx940\tx1440\pardeftab720\li1440\fi-1440\sl380
\ls1\ilvl1
\i \cf2 {\listtext	\uc0\u9702 	}JOIN
\i0 \
\ls1\ilvl1
\i {\listtext	\uc0\u9702 	}FULL OUTER JOIN
\i0 \
\ls1\ilvl1
\i {\listtext	\uc0\u9702 	}LEFT OUTER JOIN
\i0 \
\ls1\ilvl1
\i {\listtext	\uc0\u9702 	}RIGHT OUTER JOIN
\i0 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\sl380
\ls1\ilvl0
\b \cf3 {\listtext	8.	}Data Storage Formats
\b0 \cf2 \
\pard\tx940\tx1440\pardeftab720\li1440\fi-1440\sl380
\ls1\ilvl1
\i \cf2 {\listtext	\uc0\u9702 	}TextFile
\i0 \
\ls1\ilvl1
\i {\listtext	\uc0\u9702 	}SequenceFile
\i0 \
\ls1\ilvl1
\i {\listtext	\uc0\u9702 	}RCFile
\i0 \
\ls1\ilvl1
\i {\listtext	\uc0\u9702 	}ORC
\i0 \
\ls1\ilvl1
\i {\listtext	\uc0\u9702 	}Parquet
\i0 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\sl360
\ls1\ilvl0
\b \cf2 {\listtext	9.	}\'85
\b0 \
\ls1\ilvl0
\b {\listtext	10.	}\'85
\b0 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\sl380
\ls1\ilvl0
\b \cf2 {\listtext	11.	}Tuning Tips
\b0 \
\pard\tx940\tx1440\pardeftab720\li1440\fi-1440\sl380
\ls1\ilvl1
\i \cf2 {\listtext	\uc0\u9702 	}Controlling File and Split Size
\i0 \
\ls1\ilvl1
\i {\listtext	\uc0\u9702 	}Data Storage Formats
\i0 \
\ls1\ilvl1
\i {\listtext	\uc0\u9702 	}Compression
\i0 \
\ls1\ilvl1
\i {\listtext	\uc0\u9702 	}Partitioning
\i0 \
\ls1\ilvl1
\i {\listtext	\uc0\u9702 	}Bucketing
\i0 \
\ls1\ilvl1
\i {\listtext	\uc0\u9702 	}Limit
\i0 \
\ls1\ilvl1
\i {\listtext	\uc0\u9702 	}Parallel Execution
\i0 \
\pard\pardeftab720\sl380\sa380

\fs30 \cf2 \
\pard\pardeftab720\sl880\sa380

\b\fs60 \cf0 1. Introduction - What is Hive\
\pard\pardeftab720\sl380\sa380

\b0\fs30 \cf2 (\cf3 \uc0\u8682  Table of Contents\cf2 )\
(If you know Hive and Hadoop you can safely skip the introduction.)\
Apache Hive is a data warehouse system build on top of Hadoop to query 
\i Big Data
\i0 . Hive originated at {\field{\*\fldinst{HYPERLINK "https://www.facebook.com/note.php?note_id=89508453919"}}{\fldrslt \cf3 Facebook and was open sourced in August 2008}}. The challenge Facebook had to address is one faced by many companies since then. Eventually data growth in a company challenges the capabilities of deployed RDBMS or NoSQL systems. Reports and analytics start to take minutes, then hours, and eventually overlap with other queries and the whole system grinds to a halt. Another common scenario companies start processing big data with Hadoop discovers the value of making the data accessible beyond the development team capable of writing complex map-reduce jobs.\
\pard\pardeftab720\sl660\sa380

\b\fs44 \cf2 What is Big Data\
\pard\pardeftab720\sl380\sa380

\b0\fs30 \cf2 The term Big Data is freely used in this context. A colleague defined Big Data jokingly as anything beyond one million rows - Microsoft Excels row limit. The underlying point is that Big Data is a point of view and can be generalised as the point where simple solutions and deployed technology fail.\
The subsequent question is if scaling and investing heavily in it is the most economical solution. Commercial large-scale data warehouse solutions are very expensive. Furthermore, some of the data collected today, e.g. poorly structured or highly denormalized data, can be impractical to manage with these systems. The Hadoop ecosystem regularly is utilised to scale data processing in a feasible manner. Hadoop becomes either a replacement or a batch process addition to the existing infrastructure for data analysis, extraction, loading, transformation, reporting, and machine learning.\
\pard\pardeftab720\sl660\sa380

\b\fs44 \cf2 Accessing Big Data\
\pard\pardeftab720\sl380\sa380

\b0\fs30 \cf2 The downside, which Facebook encountered, was that data stored in Hadoop is inaccessible to business users. There are higher-level languages like {\field{\*\fldinst{HYPERLINK "https://pig.apache.org/"}}{\fldrslt \cf3 Pig}}, {\field{\*\fldinst{HYPERLINK "http://www.cascading.org/"}}{\fldrslt \cf3 Cascading}}, {\field{\*\fldinst{HYPERLINK "http://crunch.apache.org/"}}{\fldrslt \cf3 Crunch}} or {\field{\*\fldinst{HYPERLINK "https://github.com/twitter/scalding"}}{\fldrslt \cf3 Scalding}}. They are, however, geared towards software developers that want to avoid the verbosity of pure Java map-reduce development. Even Pig, a popular data-flow language, still requires users to learn a completely new skill. Facebook realised that most of their users already had a common skill - they knew SQL. Hive was developed to give access to data stored in Hadoop translating SQL-like statements into complex map-reduce jobs reading and processing data on large distributed scale.\
\pard\pardeftab720\sl660\sa380

\b\fs44 \cf2 Democratising Big Data\
\pard\pardeftab720\sl380\sa380

\b0\fs30 \cf2 Hive is a success story. Today, Facebook\'92s largest Hadoop cluster consists of thousands of computers providing a combined storage of 150 Petabytes - roughly 150,000,000 Gigabytes. Hive provides access to literally thousands of employees to the data running together thousands of map-reduce jobs every day using Hive. The additional training for employees knowing SQL to use Hive is minimal. Most statements can be expressed equivalently and full SQL support is coming to Hive very soon.\
\pard\pardeftab720\sl660\sa380

\b\fs44 \cf2 What Hive is not\
\pard\pardeftab720\sl380\sa380

\b0\fs30 \cf2 Hive does not use sophisticated indexes like many RDBMS which are able to answer queries in seconds. Hive queries usually take minutes or hours. However, Hive scales very well and can execute queries across data of the magnitude of Petabytes. The Hadoop ecosystem and Hive are under very active development, however, and speedups are achieved with every new iteration. In 2013 many new developments are due to be introduced, a new Hadoop framework that goes beyond map-reduce and manages resources better, and Hive improvements that will make queries on smaller data faster and interactive.\
\pard\pardeftab720\sl660\sa380

\b\fs44 \cf2 What this book will teach\
\pard\pardeftab720\sl380\sa380

\b0\fs30 \cf2 You will learn how to access data with Hive.\
\
\pard\pardeftab720\sl880\sa380

\b\fs60 \cf0 2. Getting Started - Setup Hive\
\pard\pardeftab720\sl380\sa380

\b0\fs30 \cf2 (\cf3 \uc0\u8682  Table of Contents\cf2 )\
(You can skip this section if you have access to a Hive CLI (Command Line Interface) or a Hue and Beeswax web interface to Hive.)\
Installing Hive for evaluation purposes is best done with a virtual machine provided by one of the popular Hadoop distributions:\
\pard\tx220\tx720\pardeftab720\li720\fi-720\sl380
\ls2\ilvl0
\b\fs28 \cf3 {\listtext	\'95	}{\field{\*\fldinst{HYPERLINK "http://hortonworks.com/products/hortonworks-sandbox/"}}{\fldrslt Hortonworks}}
\b0 \cf2 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\sl380
\ls2\ilvl0\cf3 {\listtext	\'95	}{\field{\*\fldinst{HYPERLINK "https://ccp.cloudera.com/display/SUPPORT/Cloudera+QuickStart+VM"}}{\fldrslt Cloudera}}\cf2 \
\ls2\ilvl0\cf3 {\listtext	\'95	}{\field{\*\fldinst{HYPERLINK "http://www.mapr.com/doc/display/MapR2/Quick+Start+-+Test+Drive+MapR+on+a+Virtual+Machine"}}{\fldrslt MapR}}\cf2 \
\pard\pardeftab720\sl380\sa380

\fs30 \cf2 The examples in this book are based on the 
\b Hortonworks Sandbox version 1.2
\b0 .\
\
\pard\pardeftab720\sl880\sa380

\b\fs60 \cf0 3. Create, Load, Select, Drop - The basics\
\pard\pardeftab720\sl380\sa380

\b0\fs30 \cf2 (\cf3 \uc0\u8682  Table of Contents\cf2 )\
Hive uses a metastore - a database - to store information about the tables it knows. Tables to Hive often are not much more than storing the information where the data is stored and how it is formatted. We do not have to know much about the metastore itself to use Hive though.\
\pard\pardeftab720\sl660\sa380

\b\fs44 \cf2 Managed Table\
\pard\pardeftab720\sl380\sa380

\b0\fs30 \cf2 Managed tables\'92s data is controlled by Hive. It will create a directory for the data on HDFS and when we drop the table it will delete the data. Later in this chapter we will see that we can manage the data ourselves with an 
\i EXTERNAL
\i0  table.\
\pard\pardeftab720\sl580\sa380

\b\fs40 \cf2 Creating an empty table\
\pard\pardeftab720\sl380\sa380

\b0\fs30 \cf2 Let us create a table, which will be a simple list of all names of all countries. Go to the Beeswax query editor and execute the following query:\
\pard\pardeftab720\sl380\sa380

\f1 \cf2 sql CREATE TABLE country_list (name STRING);
\f0 \
You can now find the table in the Beeswax table list. Once you selected the table you can view the file location on HDFS which Hive automatically selected and created for you. The table and directory are empty. Note: We can define the location of a new table as we will learn later.\
\pard\pardeftab720\sl580\sa380

\b\fs40 \cf2 Describing a table\
\pard\pardeftab720\sl380\sa380

\b0\fs30 \cf2 We can get all the information we need about a table through a query too. Go back to the query editor and execute:\
\pard\pardeftab720\sl380\sa380

\f1 \cf2 sql DESCRIBE EXTENDED country_list;
\f0 \
The result describes the schema of the table and detailed table information. This includes the location of the table and the information that it uses TextInputFormat which is default setting in the Hortonworks Hive setup.\
\pard\pardeftab720\sl580\sa380

\b\fs40 \cf2 Loading data into a table\
\pard\pardeftab720\sl380\sa380

\b0\fs30 \cf2 We want to load data to the table so we need to upload it to HDFS. We know the table has single column of country name, uses a simple text format, and Hive always uses new line characters to as row delimiters. All we need to do to add data is upload one or several files into the directory linked with the table. The files have to be formatted to have one country name on each line.\
Go back to the table list and select the 
\f1 country_list
\f0  table. Select 
\f1 View File Location
\f0  and you will be viewing the HDFS directory of where the able stores its data. Upload the 
\f1 coutry_example.tsv
\f0  file into the directory using the file upload function on the top right of the interface.\
\pard\pardeftab720\sl580\sa380

\b\fs40 \cf2 Query a table\
\pard\pardeftab720\sl380\sa380

\b0\fs30 \cf2 Hive is very flexible and checks the table\'92s data location for every query. Adding or removing data on the file system is reflected on the table. After adding the file its content is now automatically retrieved as table data. Try it out. Go back to the Beeswax table list and select the 
\f1 country_list
\f0  again. Click on browse data on the left. You should see four countries which are read from the file. You can also query the table to get the same result. Go tot he query editor and execute:\
\pard\pardeftab720\sl380\sa380

\f1 \cf2 sql SELECT * FROM country_list;
\f0 \
\pard\pardeftab720\sl580\sa380

\b\fs40 \cf2 Drop a table\
\pard\pardeftab720\sl380\sa380

\b0\fs30 \cf2 You can drop the table in the table view of Beeswax through the graphic interface. Go to the query editor and execute the equivalent query to drop the table:\
\pard\pardeftab720\sl380\sa380

\f1 \cf2 sql DROP TABLE country_list;
\f0 \
The table and the HDFS directory and file(s) are deleted.\
\pard\pardeftab720\sl660\sa380

\b\fs44 \cf2 External Table\
\pard\pardeftab720\sl380\sa380

\b0\fs30 \cf2 So far the queries and behaviour of Hive has not been very different to SQL systems. Hive\'92s separation of data location and storing the schema in a metastore enables it to create tables pointing to existing data, to read, query, and transform it, and then drop the tables without touching the original data on HDFS.\
These unmanaged tables make Hive powerful as a tool that queries outputs from other processes and systems. The terminology used is an extension of the SQL 
\f1 CREATE TABLE
\f0  statement. Let us do the above example again with an external table instead to illustrate the difference.\
\pard\pardeftab720\sl580\sa380

\b\fs40 \cf2 Create an external table\
\pard\pardeftab720\sl380\sa380

\b0\fs30 \cf2 Create the external table by simply adding 
\f1 EXTERNAL
\f0  to the statement:\
\pard\pardeftab720\sl380\sa380

\f1 \cf2 sql CREATE EXTERNAL TABLE country_list (name STRING);
\f0 \
Everything will appear to be the same as in the previous example, i.e. the table will look the same in the Beeswax interface and behave the same as the previous one. Only the detailed description reveals a difference:\

\f1 sql DESCRIBE EXTENDED country_list;
\f0 \
The table description now mentions 
\f1 tableType:EXTERNAL_TABLE
\f0  which previously was 
\f1 tableType:MANAGED_TABLE
\f0 . The managed table meant that Hive would delete the data on dropping the table - managing it. The external table type means that Hive on dropping a table will remove the schema information only - it disappears from Hive but the data remains on HDFS - Hive considers it external.\
Go ahead and try it out. If you repeat our previous example and upload the 
\f1 country_example.tsv
\f0  file to the table\'92s HDFS location the data will appears as table data as before. If you drop the table the data will remain:\

\f1 sql DROP TABLE country_list;
\f0 \
Go to the HDFS directory 
\f1 /apps/hive/warehouse/country_list
\f0  to see the data and directory still intact.\
\pard\pardeftab720\sl580\sa380

\b\fs40 \cf2 (Re)Creating an external table\
\pard\pardeftab720\sl380\sa380

\b0\fs30 \cf2 This means that we can create a table with the data on HDFS with a single statement:\
\pard\pardeftab720\sl380\sa380

\f1 \cf2 sql CREATE EXTERNAL TABLE country_list (name STRING);
\f0 \
Query the table to see that the data on HDFS has been linked back to the table:\

\f1 sql SELECT * FROM country_list;
\f0 \
\
\pard\pardeftab720\sl880\sa380

\b\fs60 \cf0 4. Setting up the example table\
\pard\pardeftab720\sl380\sa380

\b0\fs30 \cf2 (\cf3 \uc0\u8682  Table of Contents\cf2 )\
The following examples are based on World Bank data of development indicators of the last four decades. The data is freely available on the {\field{\*\fldinst{HYPERLINK "http://databank.worldbank.org/"}}{\fldrslt \cf3 World Bank website}}. The data distributed with the book for the examples has been modified for use with Hive.\
\pard\pardeftab720\sl660\sa380

\b\fs44 \cf2 Create external table\
\pard\pardeftab720\sl380\sa380

\f1\b0\fs30 \cf2 sql CREATE EXTERNAL TABLE wdi ( country_name STRING, country_code STRING, indicator_name STRING, indicator_code STRING, `1960` FLOAT, `1961` FLOAT, `1962` FLOAT, `1963` FLOAT, `1964` FLOAT, `1965` FLOAT, `1966` FLOAT, `1967` FLOAT, `1968` FLOAT, `1969` FLOAT, `1970` FLOAT, `1971` FLOAT, `1972` FLOAT, `1973` FLOAT, `1974` FLOAT, `1975` FLOAT, `1976` FLOAT, `1977` FLOAT, `1978` FLOAT, `1979` FLOAT, `1980` FLOAT, `1981` FLOAT, `1982` FLOAT, `1983` FLOAT, `1984` FLOAT, `1985` FLOAT, `1986` FLOAT, `1987` FLOAT, `1988` FLOAT, `1989` FLOAT, `1990` FLOAT, `1991` FLOAT, `1992` FLOAT, `1993` FLOAT, `1994` FLOAT, `1995` FLOAT, `1996` FLOAT, `1997` FLOAT, `1998` FLOAT, `1999` FLOAT, `2000` FLOAT, `2001` FLOAT, `2002` FLOAT, `2003` FLOAT, `2004` FLOAT, `2005` FLOAT, `2006` FLOAT, `2007` FLOAT, `2008` FLOAT, `2009` FLOAT, `2010` FLOAT, `2011` FLOAT, `2012` FLOAT ) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\\t' LOCATION '/user/sandbox/wdi';
\f0 \
The table is empty since we have not loaded any data yet. Hive created a folder at 
\f1 /user/sandbox/wdi
\f0  on HDFS for us and we can copy data there to appear in the tables.\
\pard\pardeftab720\sl660\sa380

\b\fs44 \cf2 Put the data on HDFS\
\pard\pardeftab720\sl380\sa380

\b0\fs30 \cf2 We first place the data on HDFS for Hive and then create an external table for the data. Upload the 
\f1 wdi_data.tsv.gz
\f0  file to the new HDFS 
\f1 /user/sandbox/wdi
\f0  folder. The file is now located on the distributed HDFS file system and can be read by Hadoop and Hive. The file is Gzip compressed as indicated by the 
\f1 .gz
\f0  postfix. Hive recognises this format and automatically decompresses the file at query time.\
\
\pard\pardeftab720\sl880\sa380

\b\fs60 \cf0 5. Query\
\pard\pardeftab720\sl380\sa380

\b0\fs30 \cf2 (\cf3 \uc0\u8682  Table of Contents\cf2 )\
We can check if the schema from the create statement aligns with the data we uploaded by either browsing the data from the Beeswax table interface or querying it:\
\pard\pardeftab720\sl380\sa380

\f1 \cf2 sql SELECT * FROM wdi;
\f0 \
The above statement returns all columns and all rows from the table 
\f1 wdi
\f0 .\
\pard\pardeftab720\sl660\sa380

\b\fs44 \cf2 SELECT \'85 WHERE \'85\
\pard\pardeftab720\sl380\sa380

\b0\fs30 \cf2 HiveQL supports 
\f1 WHERE
\f0  constraints on the 
\f1 SELECT
\f0  statement. Let us reduce the selection to a specific indicator. The following query returns all rows for the indicator named 
\f1 'Trade (% of GDP)'
\f0 :\
\pard\pardeftab720\sl380\sa380

\f1 \cf2 sql SELECT * FROM wdi WHERE indicator_name = 'Trade (% of GDP)';
\f0 \
We can further restrict the result to return only the country name and the indicator result of the year 2011:\

\f1 sql SELECT `country_name`, `2011` AS trade_2011 FROM wdi WHERE indicator_name = 'Trade (% of GDP)';
\f0 \
We can also exclude empty 
\f1 NULL
\f0  results for the year 2011:\

\f1 sql SELECT `country_name`, `2011` AS trade_2011 FROM wdi WHERE indicator_name = 'Trade (% of GDP)' AND `2011` IS NOT NULL;
\f0 \
\pard\pardeftab720\sl660\sa380

\b\fs44 \cf2 SELECT \'85 ORDER BY \'85\
\pard\pardeftab720\sl380\sa380

\b0\fs30 \cf2 What are the countries with the greatest and the least percentage of trade to GDP ratio? The result of the above query can be ordered by column(s). This is similarly to SQL\'92s 
\f1 ORDER BY ... (ASC|DESC)
\f0  statement. Postfixing the order statement with 
\f1 ASC
\f0  or 
\f1 DESC
\f0  will order it in ascending or descending order:\
\pard\pardeftab720\sl380\sa380

\f1 \cf2 sql SELECT `country_name`, `2011` AS trade_2011 FROM wdi WHERE indicator_name = 'Trade (% of GDP)' AND `2011` IS NOT NULL ORDER BY trade_2011 DESC;
\f0 \
The downside on ordering globally with 
\f1 ORDER BY
\f0  is that it is implemented using a single reducer. Consequently, ordering a large set of data can take a very long time.\
\pard\pardeftab720\sl660\sa380

\b\fs44 \cf2 SELECT \'85 SORT BY \'85\
\pard\pardeftab720\sl380\sa380

\b0\fs30 \cf2 I cases where you only want to approximate the order or investigate the data the 
\f1 SORT BY
\f0  statement can be used. It sorts the data only in each reducer and not globally. That can be much faster for large data sets.\
\pard\pardeftab720\sl380\sa380

\f1 \cf2 sql SELECT `country_name`, `2011` AS trade_2011 FROM wdi WHERE indicator_name = 'Trade (% of GDP)' AND `2011` IS NOT NULL SORT BY trade_2011 DESC;
\f0 \
\pard\pardeftab720\sl660\sa380

\b\fs44 \cf2 SELECT \'85 CLUSTER BY \'85\
\pard\pardeftab720\sl380\sa380

\b0\fs30 \cf2 Distributed sorting can be very helpful when you have keys to group your sets that you want to order by. For example, we may add another indicator 
\f1 'Broad money (% of GDP)'
\f0  and want the result sorted by indicator to easily split the list in two later.\
\pard\pardeftab720\sl380\sa380

\f1 \cf2 sql SELECT country_name, indicator_name, `2011` AS trade_2011 FROM wdi WHERE (indicator_name = 'Trade (% of GDP)' OR indicator_name = 'Broad money (% of GDP)') AND `2011` IS NOT NULL CLUSTER BY indicator_name;
\f0 \
The above query will return all 2011 results of all countries for the two indicators where the data is available, i.e. not null. The result will be sorted by indicator and since the input was sorted by country already the result is also sorted by country within each indicator.\
It is important to understand the computational benefit. We could have achieved this with a 
\f1 SORT BY
\f0 . However, using 
\f1 CLUSTER BY
\f0  enables Hadoop to distribute the data based on the cluster by key across all computational nodes. It is limited by the cardinality of the key though. If you have only two keys then only two reducers can work in parallel independent of you cluster size.\
Examples where 
\f1 CLUSTER BY
\f0  works excellent are where global order is irrelevant. Imagine sorting orders by category and then analyse each category of orders. You may have millions of orders and hundreds of categories. Clustering the sorting would provide a tremendous performance improvement since the sort can potentially be done by hundreds of cluster nodes in parallel.\
\pard\pardeftab720\sl660\sa380

\b\fs44 \cf2 SELECT \'85 DISTRIBUTE BY \'85\
\pard\pardeftab720\sl380\sa380

\f1\b0\fs30 \cf2 DISTRIBUTE BY
\f0  tells Hive by which column to organise the data when it is sent to the reducers. We could instead of using 
\f1 CLUSTER BY
\f0  in the previous example use 
\f1 DISTRIBUTE BY
\f0  to ensure every reducer gets a complete set of indicators.\

\f1 sql SELECT country_name, indicator_name, `2011` AS trade_2011 FROM wdi WHERE (indicator_name = 'Trade (% of GDP)' OR indicator_name = 'Broad money (% of GDP)') AND `2011` IS NOT NULL DISTRIBUTE BY indicator_name;
\f0 \
\pard\pardeftab720\sl380

\fs26 \cf2 \pard\pardeftab720\sl380\qc

\i \cf4 \cb5 \
DISTRIBUTE BY indicator_name\
\pard\pardeftab720\sl380\sa380

\i0\fs30 \cf2 \cb1 If you ran the example on the Hortonworks VM or any other setup with one reducer your query result will look like the rows are not organised by indicator names. The difference is that 
\f1 DISTRIBUTE BY
\f0  does not sort the result. It ensures that all rows with the same indicator are sent to the same reducer but it does not sort them as 
\f1 CLUSTER BY
\f0 . Consequently, all rows were sent to the one reducer you have and the output is mixed. 
\f1 CLUSTER BY
\f0  is in fact only syntactic sugar for a combination of 
\f1 DISTRIBUTE BY
\f0  and 
\f1 SORT BY
\f0 . The latter adding the local sorting on each reducer.\
\pard\pardeftab720\sl380\sa380

\f1 \cf2 sql SELECT country_name, indicator_name, `2011` AS trade_2011 FROM wdi WHERE (indicator_name = 'Trade (% of GDP)' OR indicator_name = 'Broad money (% of GDP)') AND `2011` IS NOT NULL DISTRIBUTE BY indicator_name SORT BY indicator_name;
\f0 \
\pard\pardeftab720\sl380

\fs26 \cf2 \pard\pardeftab720\sl380\qc

\i \cf4 \cb5 \
DISTRIBUTE & SORT BY indicator_name\
\pard\pardeftab720\sl380\sa380

\i0\fs30 \cf2 \cb1 The result should be equivalent with the cluster example and in each reducer the rows were sorted.\
[Here be dragons]\
\
\pard\pardeftab720\sl880\sa380

\b\fs60 \cf0 7. Joining Tables\
\pard\pardeftab720\sl380\sa380

\b0\fs30 \cf2 (\cf3 \uc0\u8682  Table of Contents\cf2 )\
\pard\pardeftab720\sl660\sa380

\b\fs44 \cf2 JOIN\
\pard\pardeftab720\sl380\sa380

\b0\fs30 \cf2 Joins are very common operations to combine related tables and 
\i join
\i0  them on a shared value.\
NoSQL and big data architectures tend to diverge from traditional table designs and normalisations to reduce JOINs. Until recently duplication of columns in tables was seen as wasteful and hard to change. The 
\f1 wdi
\f0  table has a country name and country code column. Traditionally tables would only contain an identifier/code and we would store all country relevant information including the full name in a separate country table. Since storage has become cheap and plentiful we can observe a tendency to duplicate commonly used information in tables like country name to reduce the need for JOINs. This simplifies daily operations, analytics, and saves computation on JOINs on the expense of storage.\
However, there are plenty of examples where JOINs are needed. The most common example is the inner JOIN. One situation may be that we have a list of countries like 
\f1 country_list
\f0  table from our introduction example. The question be that we want to know all indicators for this subset of countries for 2011. The query would then be an inner JOIN of 
\f1 country_list
\f0  with 
\f1 wdi
\f0  on the country names to select the rows of interest from 
\f1 wdi
\f0 . Additionally we can add a 
\f1 WHERE
\f0  clause to exclude rows without value for 2011 and order the result nicely by indicators and countries to make it more readable.\
\pard\pardeftab720\sl380\sa380

\f1 \cf2 sql SELECT w.country_name, w.indicator_name, w.`2011` FROM country_list c JOIN wdi w ON w.country_name = c.name WHERE `2011` IS NOT NULL ORDER BY w.indicator_name, w.country_name;
\f0 \
We declared the aliases 
\b c
\b0  and 
\b w
\b0  after mentioning the tables in the query to simplify the query 
\f1 ... FROM country_list c JOIN wdi w ...
\f0 , which allows to reference them then like 
\f1 w.indicator_name
\f0  instead of 
\f1 wdi.indicator_name
\f0 .\
\pard\pardeftab720\sl380

\fs26 \cf2 \pard\pardeftab720\sl380\qc

\i \cf4 \cb5 \
(INNER) JOIN\
\pard\pardeftab720\sl380\sa380

\i0\fs30 \cf2 \cb1 The results above show that only rows were used that satisfy the JOIN, i.e. have a matching country name in both 
\f1 wdi
\f0  and 
\f1 country_list
\f0 .\
\pard\pardeftab720\sl660\sa380

\b\fs44 \cf2 FULL OUTER JOIN\
LEFT OUTER JOIN\
RIGHT OUTER JOIN\
Optimising Joins\
\pard\pardeftab720\sl580\sa380

\fs40 \cf2 Map JOIN\
\pard\pardeftab720\sl380\sa380

\f1\b0\fs30 \cf2 SET hive.auto.convert.join=true;
\f0 \
\pard\pardeftab720\sl580\sa380

\b\fs40 \cf2 Avoid NULL values in JOIN\
\pard\pardeftab720\sl380\sa380

\f1\b0\fs30 \cf2 ... FROM y JOIN x ON y.a=x.a AND y.a IS NOT NULL ...
\f0 \
\
\pard\pardeftab720\sl880\sa380

\b\fs60 \cf0 8. Data Storage Formats\
\pard\pardeftab720\sl380\sa380

\b0\fs30 \cf2 (\cf3 \uc0\u8682  Table of Contents\cf2 )\
\pard\pardeftab720\sl660\sa380

\b\fs44 \cf2 TextFile\
\pard\pardeftab720\sl580\sa380

\fs40 \cf2 Delimiters and Gotchas\
Compression\
\pard\pardeftab720\sl660\sa380

\fs44 \cf2 SequenceFile\
RCFile\
ORC\
Parquet\
\pard\pardeftab720\sl380\sa380

\b0\fs30 \cf2 \
\pard\pardeftab720\sl880\sa380

\b\fs60 \cf0 9. Partitioning\
\pard\pardeftab720\sl380\sa380

\b0\fs30 \cf2 (\cf3 \uc0\u8682  Table of Contents\cf2 )\
date, region, country, category, boolean \'85 IS NULL.\
[Here be dragons]\
}